https://a16z.com/emerging-architectures-for-llm-applications/

大型语言模型正开始成为构建软件的强大新基础资源。但是，由于它太新了——而且与其他普通计算资源太不一样——因此并不容易上手使用。

在这篇文章中，我们将分享一个新的LLM应用程序栈参考架构。它展示了我们看到的人工智能初创公司和成熟的科技公司使用的最常见的系统、工具和设计模式。这个堆栈仍然很早期，并且可能会随着底层技术的进步而发生重大变化，但我们希望它对现在使用LLM的开发人员是一个有用的参考。

本作品基于与AI创业公司创始人和工程师的对话。我们特别依赖来自以下方面的意见：泰德·本森、哈里森·蔡斯、本·菲尔希曼、阿里·戈西、拉扎·哈比布、安德烈·卡帕西、格雷格·科根、杰瑞·刘、莫因·纳迪姆、迭戈·奥本海默、施雷亚·拉杰帕尔、伊翁·斯托伊卡、丹尼斯·许、马泰·扎哈里亚和贾里德·雷奇。谢谢你的帮助！

# 栈
这是我们当前的LLM应用程序栈视图：

这里列出了每个项目的链接，供您快速参考：

数据管道嵌入模型矢量数据库游乐场编排API/插件LLM缓存
Databricks OpenAI松果OpenAI浪链Serp Redis
气流聚合Weaviate nat.dev LlamaIndex Wolfram SQLite
非结构化Hugging Face ChromaDB Humanloop ChatGPT Zapier GPTCache
pg矢量
日志记录/LMops验证应用程序托管LLM API（专有）LLM API（开放）云提供商意见云
权重和偏置护栏VercelOpenAI Hugging Face AWS Databricks
MLflow拒绝蒸汽船人类复制GCP任意尺度
提示层微软指南流化Azure马赛克
Helicone LMQL模态核心编织模态
RunPod

有许多不同的方法可以使用LLM进行构建软件，包括从头开始训练模型、微调开源模型或使用托管API。我们在这里展示的堆栈是基于上下文学习(https://en.wikipedia.org/wiki/In-context_learning_(natural_language_processing))，这是我们看到大多数开发人员开始使用的设计模式（现在只有在基础模型中才可能使用）。

下一节将简要说明此模式，有经验的LLM开发人员可以跳过本节。

# 设计模式：上下文学习

上下文内学习的核心思想是使用现成的LLM（即，没有任何微调），然后通过对私有“上下文”数据的巧妙提示和条件反射来控制它们的行为。

例如，假设您正在构建一个聊天机器人来回答有关一组法律文件的问题。采用一种简单的方法，您可以将所有文档粘贴到ChatGPT或GPT-4提示符中，然后在最后问一个关于它们的问题。这可能适用于非常小的数据集，但它无法扩展。最大的GPT-4模型只能处理大约50页的输入文本，当你接近这个极限（称为上下文窗口）时，性能（通过推理时间和准确性来衡量）会严重下降。

In-Context Learning通过一个巧妙的技巧解决了这个问题：它不是发送每个LLM提示的所有文档，而是只发送一小部分最相关的文档。而最相关的文档是在...你猜对了...LLMs的帮助下确定的。

**在非常抽象的层面上，工作流程可分为三个阶段**：

* **数据预处理/嵌入**（Data Preprocessing / Embedding）：此阶段涉及存储私有数据（在我们的示例中，是法律文档），以便稍后检索。通常，文档被分成块，通过嵌入模型传递，然后存储在称为向量数据库的专用数据库中。
* **提示构造/检索**（Prompt Construction / Retrieval）：当用户提交一个查询（在本例中是一个法律问题）时，应用程序构造一系列提示以提交给语言模型。处理后的提示词通常结合了一个由开发人员硬编码的提示模板、称为少量示例的有效输出的示例、从外部API检索到的任何必要信息、以及从向量数据库检索到的一组相关文档。
* **提示执行/推理**（Prompt Execution / Inference）：一旦提示词被生成，它们将提交给一个已经被预训练的LLM进行推理，包括专有模型API和开源或自训练的模型。一些开发人员还在此阶段添加了日志、缓存和验证等操作系统。

这看起来是很多工作，但通常比训练或微调LLM本身更简单。不需要一个专门的ML工程师团队来进行In-Context Learning。也不需要托管自己的基础设施或从OpenAI购买昂贵的专用实例。这种模式有效地将AI问题简化为数据工程问题，大多数创业公司和大公司已经知道如何解决。对于相对较小的数据集，它的表现也往往优于微调，因为一条特定的信息需要在训练集中至少出现10次，然后LLM才能通过微调记住它，并且可以近实时地合并新数据。

关于In-Context Learning的最大问题之一是：如果我们只是改变基础模型来增加上下文窗口，会发生什么？这确实是可能的，而且它是一个活跃的研究领域（例如，参见鬣狗论文或最近的帖子）。但这伴随着一些权衡——主要是推理的成本和时间与提示的长度成二次方的比例。如今，即使是线性缩放（最好的理论结果）对于许多应用程序来说也是成本高昂的。以当前的API费率，单个超过10,000个页面的GPT-4查询将花费数百美元。因此，我们不期望基于扩展的上下文窗口对堆栈进行大规模更改，但我们将在帖子的正文中对此进行更多评论。

如果你想深入了解In-Context Learning，AI canon中有很多很棒的资源（特别是“使用LLM进行构建的实用指南”部分）。在本篇文章的剩余部分中，我们将使用上面的工作流程作为指导，遍历引用堆栈。


## 数据预处理/嵌入（Data Preprocessing / Embedding）

LLM应用程序的**上下文数据**（Contextual data）包括文本文档、PDF、还有如CSV或SQL表的结构化数据。针对这些数据的数据加载和转换解决方案在我们采访过的开发人员中差异很大。大多数使用传统的ETL工具，如Databricks或Airflow。有些还使用内置于编排框架中的文档加载器，如LangChain（由非结构化提供支持）和LlamaIndex（由Llama Hub提供支持）。不过，我们认为堆栈中的这一块相对来说还不够发达，而专门为LLM应用程序构建的数据复制解决方案也有机会。

对于**嵌入**（Embedding），大多数开发人员使用OpenAI API，特别是使用text-embedding-adam模型。它易于使用（特别是当你已经在使用其他OpenAI API时），提供了相当好的结果，并且变得越来越便宜。一些规模较大的企业也在探索Cohere，它将他们的产品精力更多地集中在embedding上，在某些场景下具有更好的性能。对于喜欢开源的开发人员来说，来自Hugging Face的Sentence Transformers库是一个标准。还可以根据不同的用例创建不同类型的嵌入；这是今天的小众实践，但是一个有前途的研究领域。

从系统的角度来看，预处理管道中最重要的部分是向量数据库。它负责高效地存储、比较和检索多达数十亿个嵌入（即向量）。我们在市场上看到的最常见的选择是松果。它是默认的，因为它完全由云托管，因此易于入门，并且具有大型企业在生产中所需要的许多功能（例如，良好的大规模性能、SSO和正常运行时间SLA）。

不过，有大量的向量数据库可用。值得注意的是：

* **像Weaviate、Vespa和Qdrant这样的开源系统**：它们通常提供出色的单节点性能，并且可以针对特定的应用进行定制，因此它们很受喜欢构建定制平台的经验丰富的AI团队的欢迎。
* **像Chroma和Faiss这样的本地矢量管理库**：他们有很好的开发者经验，对于小型应用程序和开发实验很容易启动。它们不一定能替代大规模的完整数据库。
* **像pgvector这样的OLTP扩展**：对于那些看到每个数据库形状的漏洞并试图插入Postgres的开发人员，或者那些从单个云提供商处购买其大部分数据基础架构的企业，这是一个很好的向量支持解决方案。从长远来看，不清楚将向量和标量工作负载紧密耦合是否有意义。

展望未来，大多数开源矢量数据库公司都在开发云产品。我们的研究表明，在云中实现强大的性能，跨越可能的用例的广阔设计空间，是一个非常困难的问题。因此，可选的集合在短期内可能不会发生巨大变化，但在长期内很可能会发生改变。关键问题是，矢量数据库是否将类似于它们的OLTP和OLAP对手，围绕一两个流行的系统进行整合。

另一个开放的问题是，随着大多数模型的可用上下文窗口的增长，嵌入和向量数据库将如何演变。可以说，嵌入将变得不那么相关，因为上下文数据可以直接放到提示符中。然而，来自专家对这个主题的反馈表明了相反的情况——随着时间的推移，嵌入管道可能会变得更加重要。大上下文窗口是一个强大的工具，但它们也带来了巨大的计算成本。因此，有效利用它们成为当务之急。我们可能会开始看到不同类型的嵌入模型变得流行，直接针对模型相关性进行训练，以及旨在启用和利用这一点的向量数据库。


## 提示构造/检索（Prompt Construction / Retrieval）

提示LLM和整合上下文数据的策略变得越来越复杂，并且作为产品差异化的来源越来越重要。大多数开发人员开始新项目时都会尝试使用简单的提示，这些提示包括直接指令（零输出提示）或可能的一些示例输出（零输出提示）。这些提示通常会提供良好的结果，但达不到生产部署所需的准确级别。

下一级的提示柔术被设计为在某些真实源中为模型响应打基础，并提供模型没有训练的外部环境。提示工程指南至少列出了12种（!）更高级的提示策略，包括思维链、自我一致性、生成知识、思维树、定向刺激和许多其他策略。这些策略还可以结合使用来支持不同的LLM用例，如文档问题解答、聊天机器人等。

这就是像LangChain和LlamaIndex这样的编排框架大放异彩的地方。它们抽象出了提示链的许多细节；与外部API接口（包括确定何时需要API调用）；从向量数据库中检索上下文数据；以及跨多个LLM调用维护内存。它们还为上面提到的许多常见应用程序提供了模板。它们的输出是一个提示，或一系列提示，提交给语言模型。这些框架在爱好者和希望让应用程序起步的初创公司中被广泛使用，其中LangChain是领导者。

LangChain仍然是一个相对较新的项目（目前在0.0.201版本上），但我们已经开始看到用它构建的应用程序进入生产环境。一些开发人员，特别是LLM的早期采用者，更喜欢在生产中切换到原始Python，以消除额外的依赖。但是，我们预计这种DIY方法在大多数用例中会随着时间的推移而衰落，这与传统的Web应用堆栈类似。

眼尖的读者会注意到编排框中有一个看似奇怪的条目：ChatGPT。在其正常的化身中，ChatGPT是一个应用程序，而不是一个开发人员工具。但它也可以作为API访问。而且，如果你眯着眼睛看，它可以执行与其他编排框架相同的一些功能，例如：抽象出对定制提示的需求；维护状态；以及通过插件、API或其他来源检索上下文数据。虽然不是这里列出的其他工具的直接竞争对手，但ChatGPT可以被认为是一种替代解决方案，并且它最终可能成为一种可行的、简单的快速构建替代方案。


## 提示执行/推理（Prompt Execution / Inference）

如今，OpenAI在语言模型中处于领先地位。几乎每个我们交谈过的开发人员都使用OpenAI API启动新的LLM应用程序，通常使用gpt-4或gpt-4-32k模型。这提供了应用程序性能的最佳情况，并且易于使用，因为它在广泛的输入域上运行，通常不需要微调或自托管。

**当项目投入生产并开始扩大规模时，一组更广泛的选择就会发挥作用。我们听到的一些常见说法包括：**

* **切换到gpt-3.5-turbo**：它比GPT-4便宜约50倍，速度明显更快。许多应用程序不需要GPT-4级别的准确性，但确实需要低延迟推理和免费用户的成本效益支持。
* **与其他专有供应商进行试验（尤其是Anthropic的Claude模型）**：Claude提供了快速推理、GPT-3.5级精度、为大型客户提供更多定制选项，以及高达100k的上下文窗口（尽管我们发现准确性随着输入的长度而下降）。
* **将一些请求分类到开源模型**：这在大量B2C用例（如搜索或聊天）中尤其有效，在这些用例中，查询复杂度差异很大，需要廉价地服务免费用户。
  * 这通常在微调开源基础模型时最有意义。在本文中，我们不会深入讨论该工具堆栈，但像Databricks、Anyscale、Mosaic、Modal和RunPod这样的平台正在被越来越多的工程团队使用。
  * 有多种推理选项可用于开源模型，包括来自Hugging Face和Replicate的简单API接口；来自主要云提供商的原始计算资源；以及像上面列出的更有意见的云产品。

**开源模型**目前落后于专有产品，但差距正在开始缩小。来自Meta的LLaMa模型为开源准确性设置了新的标准，并启动了一系列变体。由于LLaMa被授权仅用于研究用途，一些新的提供商已经介入来训练替代的基础模型（例如，联合、马赛克、猎鹰、米斯特拉尔）。Meta也在讨论LLaMa 2的真正开源版本。

当（不是如果）开源LLM达到与GPT-3.5相媲美的精度水平时，我们预计会看到一个类似文本的稳定扩散时刻，包括大规模的实验、共享和微调模型的生产。像Replicate这样的托管公司已经在添加工具，使软件开发人员更容易使用这些模型。开发人员越来越相信，较小的、经过微调的模型可以在狭窄的用例中达到最先进的精度。

我们交谈过的大多数开发人员还没有深入了解LLM的操作工具。缓存是相对常见的，通常基于Redis，因为它可以提高应用程序响应时间和成本。像Weights & Biases和MLflow（从传统机器学习中移植过来的），或者TipsLayer和Helicone（专门为LLM构建的）这样的工具也得到了相当广泛的应用。它们可以记录、跟踪和评估LLM输出，通常用于改进快速构建、调整流水线或选择模型。还有一些新的工具正在开发，用于验证LLM输出（例如，Guardrails）或检测即时注入攻击（例如，Rebuff）。大多数这些操作工具都鼓励使用自己的Python客户端来进行LLM调用，因此，随着时间的推移，看看这些解决方案是如何共存的将是很有趣的。

最后，LLM应用程序的静态部分（即模型之外的所有内容）也需要托管在某个地方。到目前为止，我们看到的最常见的解决方案是标准选项，如Vercel或主要的云提供商。然而，有两个新的类别正在出现。像Steamship这样的初创公司为LLM应用提供了端到端托管，包括编排（LangChain）、多租户数据上下文、异步任务、向量存储和密钥管理。像Anyscale和Modal这样的公司允许开发人员在一个地方托管模型和Python代码。

# 代理

这个参考架构中缺少的最重要的组件是**AI代理框架**（AI agent frameworks）。AutoGPT被描述为“让GPT-4完全自治的实验性开源尝试”，是今年春天历史上增长最快的Github repo，几乎今天所有的AI项目或初创公司都包含了某种形式的代理。

与我们交谈的大多数开发人员都对代理的潜力感到难以置信的兴奋。我们在这篇文章中描述的上下文学习模式可以有效地解决幻觉和数据新鲜度问题，以便更好地支持内容生成任务。另一方面，智能体赋予了AI应用一套全新的能力：解决复杂问题，对外界采取行动，以及在部署后从经验中学习。他们通过高级推理/规划、工具使用和记忆/递归/自我反思的结合来做到这一点。

因此，代理有可能成为LLM应用架构的核心部分（甚至接管整个堆栈，如果你相信递归自我完善）。而像LangChain这样的现有框架已经融入了一些代理的概念。只有一个问题：代理还不能真正工作。如今，大多数代理框架都处于概念验证阶段，能够进行令人难以置信的演示，但还不能实现可靠的、可重复的任务完成。我们正在密切关注它们在不久的将来会如何发展。

# 后记

预训练的AI模型代表了自互联网以来软件领域最重要的架构变革。它们使个人开发者能够在几天内构建出令人难以置信的AI应用程序，这超过了大型团队花费数月时间构建的有监督的机器学习项目。

我们在这里提出的工具和模式很可能是集成LLM的起点，而不是最终状态。我们将在发生重大变化时更新（例如，向模型训练的转变），并在有意义的地方发布新的参考架构。如果您有任何反馈或建议，请联系。
